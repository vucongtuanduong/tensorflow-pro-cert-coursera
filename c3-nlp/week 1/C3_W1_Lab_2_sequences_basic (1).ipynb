{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"461e9c141d4b40dda8ee24feffe6eb91","deepnote_cell_type":"markdown"},"source":"<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C3/W1/ungraded_labs/C3_W1_Lab_2_sequences_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","block_group":"c4afbcb4a2e94f46a45ed55765a82bef"},{"cell_type":"markdown","metadata":{"id":"1SmE2CODfmmL","cell_id":"dfa346ed2ee647a68adab13e9a2036f7","deepnote_cell_type":"markdown"},"source":"# Ungraded Lab: Generating Sequences and Padding\n\nIn this lab, you will look at converting your input sentences into a sequence of tokens. Similar to images in the previous course, you need to prepare text data with uniform size before feeding it to your model. You will see how to do these in the next sections.","block_group":"0e6320624c574deba5a853e7042faee9"},{"cell_type":"markdown","metadata":{"id":"JiFUJg-lmTm6","cell_id":"5c10af9dfd2841309969cf22f4f94b8d","deepnote_cell_type":"markdown"},"source":"## Text to Sequences\n\nIn the previous lab, you saw how to generate a `word_index` dictionary to generate tokens for each word in your corpus. You can then use the result to convert each of the input sentences into a sequence of tokens. That is done using the [`texts_to_sequences()`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences) method as shown below.","block_group":"6e4387ac6f1843d192a866cbab1e1622"},{"cell_type":"code","metadata":{"id":"ArOPfBwyZtln","source_hash":"3b741b87","execution_start":1695547573393,"execution_millis":4053,"deepnote_to_be_reexecuted":false,"cell_id":"4ff42c466d314aac8e5a76f70b9055fd","deepnote_cell_type":"code"},"source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Define your input texts\nsentences = [\n    'I love my dog',\n    'I love my cat',\n    'You love my dog!',\n    'Do you think my dog is amazing?'\n]\n\n# Initialize the Tokenizer class\ntokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n\n# Tokenize the input sentences\ntokenizer.fit_on_texts(sentences)\n\n# Get the word index dictionary\nword_index = tokenizer.word_index\n\n# Generate list of token sequences\nsequences = tokenizer.texts_to_sequences(sentences)\n\n# Print the result\nprint(\"\\nWord Index = \" , word_index)\nprint(\"\\nSequences = \" , sequences)","block_group":"7cbea7d7fe534df3ab393fed723ea913","execution_count":1,"outputs":[{"name":"stderr","text":"2023-09-24 09:26:13.357523: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-09-24 09:26:13.554590: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-09-24 09:26:13.554623: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-09-24 09:26:13.586135: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-09-24 09:26:15.006188: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-09-24 09:26:15.006261: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-09-24 09:26:15.006270: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n\nWord Index =  {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n\nSequences =  [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"id":"z56pEkF2p8c-","cell_id":"15ac928d40a34452ac85bca2cd5645ff","deepnote_cell_type":"markdown"},"source":"## Padding\n\nAs mentioned in the lecture, you will usually need to pad the sequences into a uniform length because that is what your model expects. You can use the [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) for that. By default, it will pad according to the length of the longest sequence. You can override this with the `maxlen` argument to define a specific length. Feel free to play with the [other arguments](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences#args) shown in class and compare the result.","block_group":"e45a1d55ec2d439fbcf10f043f71595c"},{"cell_type":"code","metadata":{"id":"qljgx1eSlEse","source_hash":"c44499ab","execution_start":1695547577266,"execution_millis":40,"deepnote_to_be_reexecuted":false,"cell_id":"ad2288d44cab4c4082f4b710cd6d9fbc","deepnote_cell_type":"code"},"source":"# Pad the sequences to a uniform length\npadded = pad_sequences(sequences, maxlen=5)\n\n# Print the result\nprint(\"\\nPadded Sequences:\")\nprint(padded)","block_group":"3908225b3e5e4184a0455f0465e37895","execution_count":2,"outputs":[{"name":"stdout","text":"\nPadded Sequences:\n[[ 0  5  3  2  4]\n [ 0  5  3  2  7]\n [ 0  6  3  2  4]\n [ 9  2  4 10 11]]\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"id":"btEb9jI0k7Ip","cell_id":"00a3f1076efd4cc18bd0277c631c9f77","deepnote_cell_type":"markdown"},"source":"## Out-of-vocabulary tokens\n\nNotice that you defined an `oov_token` when the `Tokenizer` was initialized earlier. This will be used when you have input words that are not found in the `word_index` dictionary. For example, you may decide to collect more text after your initial training and decide to not re-generate the `word_index`. You will see this in action in the cell below. Notice that the token `1` is inserted for words that are not found in the dictionary.","block_group":"a5496f9dc7294d81a49204f52c505304"},{"cell_type":"code","metadata":{"id":"4fW1NWTok72V","source_hash":"cfa1c942","execution_start":1695547577277,"execution_millis":26,"deepnote_to_be_reexecuted":false,"cell_id":"fe1a375a33524ba0acc033cfb60754f2","deepnote_cell_type":"code"},"source":"# Try with words that the tokenizer wasn't fit to\ntest_data = [\n    'i really love my dog',\n    'my dog loves my manatee'\n]\n\n# Generate the sequences\ntest_seq = tokenizer.texts_to_sequences(test_data)\n\n# Print the word index dictionary\nprint(\"\\nWord Index = \" , word_index)\n\n# Print the sequences with OOV\nprint(\"\\nTest Sequence = \", test_seq)\n\n# Print the padded result\npadded = pad_sequences(test_seq, maxlen=10)\nprint(\"\\nPadded Test Sequence: \")\nprint(padded)","block_group":"1e027b416917437490b787cf50130ac0","execution_count":3,"outputs":[{"name":"stdout","text":"\nWord Index =  {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n\nTest Sequence =  [[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n\nPadded Test Sequence: \n[[0 0 0 0 0 5 1 3 2 4]\n [0 0 0 0 0 2 4 1 2 1]]\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"id":"UBlQIPBqskAJ","cell_id":"8ef030d8f7ab459291896a900921ef9a","deepnote_cell_type":"markdown"},"source":"This concludes another introduction to text data preprocessing. So far, you've just been using dummy data. In the next exercise, you will be applying the same concepts to a real-world and much larger dataset.","block_group":"e3d6c746dc094e5d9314b1e60a7d3a36"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=56659abd-468c-479e-9cc1-59c4e3afb148' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"C3_W1_Lab_2_sequences_basic.ipynb","provenance":[{"file_id":"https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/adding_C3/C3/W1/ungraded_labs/C3_W1_Lab_2_sequences_basic.ipynb","timestamp":1642431659610}],"toc_visible":true,"private_outputs":true,"collapsed_sections":[]},"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","version":"3.7.4","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"443084e3ec5e4439b322019474ac6d4d","deepnote_execution_queue":[]}}